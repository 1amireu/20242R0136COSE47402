{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wQNFjQvuZGa",
        "outputId": "4ebe9561-873d-4e0f-9e51-44045cd64296",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-8k5ikxzf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-8k5ikxzf\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=bc5ca2842a422e6840f61dd5d38f587cc9f516d981d77538d05cd1de3ea56863\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p4z9ne01/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = \"/content/drive/My Drive/Final Project DL/images/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuNWPpCfwOMS",
        "outputId": "08095016-2587-427c-bccb-c234b5d9e098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from torchvision.datasets import Food101\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load CLIP Model\n",
        "def load_clip_model(device='cpu', backbone='ViT-B/16'):\n",
        "    clip_model, preprocess = clip.load(backbone, device=device)\n",
        "    return clip_model, preprocess\n",
        "\n",
        "# Process Image with CLIP\n",
        "def process_image_with_clip(image_path, clip_model, preprocess, device='cpu'):\n",
        "    image = Image.open(image_path)\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)  # Move image tensor to the device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image_input)\n",
        "    return image_features\n",
        "\n",
        "# Predict the food class from CLIP\n",
        "def predict_food_class(image_features, clip_model, dataset, device='cpu'):\n",
        "    class_names = dataset.classes\n",
        "    text_inputs = torch.cat([clip.tokenize(class_name) for class_name in class_names]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_inputs)\n",
        "\n",
        "    similarity = cosine_similarity(image_features.cpu().numpy(), text_features.cpu().numpy())\n",
        "    predicted_class_index = similarity.argmax()\n",
        "    predicted_class_name = class_names[predicted_class_index]\n",
        "    return predicted_class_name\n",
        "\n",
        "# Main function for CLIP-based prediction\n",
        "def main(image_path, clip_model, preprocess, dataset, device='cpu'):\n",
        "    image_features = process_image_with_clip(image_path, clip_model, preprocess, device)\n",
        "    predicted_food_class = predict_food_class(image_features, clip_model, dataset, device)\n",
        "    return predicted_food_class\n"
      ],
      "metadata": {
        "id": "3OvDDLY02U7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import FlaxAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load the T5 model and tokenizer\n",
        "MODEL_NAME_OR_PATH = \"flax-community/t5-recipe-generation\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, use_fast=True)\n",
        "model = FlaxAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "\n",
        "prefix = \"items: \"\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 512,\n",
        "    \"min_length\": 128,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"do_sample\": True,\n",
        "    \"top_k\": 60,\n",
        "    \"top_p\": 0.95\n",
        "}\n",
        "\n",
        "def generation_function(texts):\n",
        "    _inputs = texts if isinstance(texts, list) else [texts]\n",
        "    inputs = [prefix + inp for inp in _inputs]\n",
        "    inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"jax\"\n",
        "    )\n",
        "\n",
        "    input_ids = inputs.input_ids\n",
        "    attention_mask = inputs.attention_mask\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        **generation_kwargs\n",
        "    )\n",
        "    generated = output_ids.sequences\n",
        "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "def process_recipe_output(recipe_text):\n",
        "    # Use regex to capture title, ingredients, and directions sections\n",
        "    title = \"\"\n",
        "    ingredients = \"\"\n",
        "    directions = \"\"\n",
        "\n",
        "   # Define regex patterns for extracting sections\n",
        "    title_pattern = r\"(?<=^title:)(.*?)(?=ingredients:|directions:|$)\"\n",
        "    ingredients_pattern = r\"(?<=ingredients:)(.*?)(?=directions:|$)\"\n",
        "    directions_pattern = r\"(?<=directions:)(.*)\"\n",
        "    # Extract sections using regex\n",
        "    title_match = re.search(title_pattern, recipe_text, re.IGNORECASE)\n",
        "    ingredients_match = re.search(ingredients_pattern, recipe_text, re.IGNORECASE)\n",
        "    directions_match = re.search(directions_pattern, recipe_text, re.IGNORECASE)\n",
        "\n",
        "    if title_match:\n",
        "        title = title_match.group(1).strip()\n",
        "    if ingredients_match:\n",
        "        ingredients = ingredients_match.group(1).strip()\n",
        "    if directions_match:\n",
        "        directions = directions_match.group(1).strip()\n",
        "\n",
        "    return title, ingredients, directions\n",
        "\n",
        "# Function to generate recipe from the dish name and separate it into title, ingredients, and directions\n",
        "def generate_recipe_from_dish(dish_name):\n",
        "    recipe = generation_function(dish_name)\n",
        "    print(f\"Generated Recipe for {dish_name}:\\n\")\n",
        "\n",
        "    # Process the output into separate sections\n",
        "    title, ingredients, directions = process_recipe_output(recipe)\n",
        "\n",
        "    # Print the structured output\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Ingredients: {ingredients}\")\n",
        "    print(f\"Directions: {directions}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vMSO-bfeJOMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recipe_from_image(image_path, clip_model, preprocess, dataset, device='cpu'):\n",
        "    # Step 1: Predict the dish name using CLIP\n",
        "    predicted_dish_name = main(image_path, clip_model, preprocess, dataset, device)\n",
        "\n",
        "    # Step 2: Generate the recipe for the predicted dish name using t5-recipe-generation\n",
        "    generate_recipe_from_dish(predicted_dish_name)\n",
        "\n",
        "# Usage example:\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = load_clip_model(device)\n",
        "dataset = Food101(root='/content/food101', split='train', download=True)\n",
        "\n",
        "# Provide an image path of a dish for testing\n",
        "image_path = base_path + \"bolognese.jpg\"\n",
        "generate_recipe_from_image(image_path, clip_model, preprocess, dataset, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCsL3RHW2cx8",
        "outputId": "4b84bdb1-ddfd-43ac-c22a-bd0da3e3953d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Recipe for spaghetti_bolognese:\n",
            "\n",
            "Dish Name: spaghetti with bolognese and pecorino romano\n",
            "Ingredients: spaghetti from ibarra spaghetti parma spaghetti from your favorite pasta recipe\n",
            "Directions: prepare the pasta and serve. cook the pasta in plenty of salted water. drain and serve with a little extra pecorine romano. you can also do this in a pot with lots of garlic and olive oil and steam for a moment or two. serve with an italian bread. a la cartera ripe pecoriola is made from the same pasta from cippolini, such as la chianti. see the notes.\n"
          ]
        }
      ]
    }
  ]
}