{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wQNFjQvuZGa",
        "outputId": "56951bfc-57ef-4696-cb8c-11f23be8c1cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-v4zi1wr4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-v4zi1wr4\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = \"/content/drive/My Drive/Final Project DL/images/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuNWPpCfwOMS",
        "outputId": "64e87711-781e-4e08-8a64-7c2614dbb8a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tjcAvUAoQrA",
        "outputId": "fdea8d1a-ef7d-47cd-8d63-5ee5b60f1104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dish name and ingredients based on the image.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import clip\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from torchvision import transforms\n",
        "\n",
        "# Step 1: Load CLIP Model (for image feature extraction)\n",
        "def load_clip_model(device='cpu', backbone='ViT-B/16'):\n",
        "    clip_model, preprocess = clip.load(backbone, device=device)\n",
        "    return clip_model, preprocess\n",
        "\n",
        "# Step 2: Process Image to Extract Features using CLIP\n",
        "def process_image_with_clip(image_path, clip_model, preprocess, device='cpu'):\n",
        "    from PIL import Image\n",
        "    image = Image.open(image_path)\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)  # Move image tensor to the device\n",
        "\n",
        "    # Extract image features using CLIP\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image_input)\n",
        "    return image_features\n",
        "\n",
        "# Step 3: Load T5 Model (for text generation)\n",
        "def load_t5_model(device='cpu'):\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)  # Move T5 model to device\n",
        "    return tokenizer, model\n",
        "\n",
        "# Step 4: Generate Dish Name and Ingredients using T5\n",
        "def generate_description(image_features, tokenizer, model, device='cpu'):\n",
        "    # Convert the image features to text prompt\n",
        "    # For now, use a static prompt.\n",
        "    input_prompt = \"Generate dish name and ingredients based on the image.\"\n",
        "\n",
        "    # Tokenize the input prompt for T5\n",
        "    input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Generate the description using T5\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(input_ids, max_length=50)\n",
        "\n",
        "    # Decode the output into text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Step 5: Putting it all together\n",
        "def main(image_path, device='cpu'):\n",
        "    # Load models\n",
        "    clip_model, preprocess = load_clip_model(device)\n",
        "    tokenizer, model = load_t5_model(device)\n",
        "\n",
        "    # Process the image with CLIP\n",
        "    image_features = process_image_with_clip(image_path, clip_model, preprocess, device)\n",
        "\n",
        "    # Generate dish description (name and ingredients)\n",
        "    description = generate_description(image_features, tokenizer, model, device)\n",
        "    return description\n",
        "\n",
        "# Example usage\n",
        "image_path = base_path + \"bolognese.jpg\"\n",
        "description = main(image_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(description)\n"
      ]
    }
  ]
}